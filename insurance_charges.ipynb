{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1S4MlB0GTXmhZKUpEaXo6CpMGkmS33p8c",
      "authorship_tag": "ABX9TyMOMpgNE2bkfV43ZaNqcaOx"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Load Libraries"
      ],
      "metadata": {
        "id": "WcIqM039BmXe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UTu-q5wo_9vI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, Lasso\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import all necessary libraries to run prediction objective"
      ],
      "metadata": {
        "id": "eI-2HGqvCtEr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "7aBv5cuwCzZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset from Google Drive\n",
        "os.chdir('/content/drive/MyDrive/Colab/Datasets/')\n",
        "df = pd.read_csv('insurance.csv')\n",
        "\n",
        "# Check for null values\n",
        "df.isnull().sum()\n",
        "\n",
        "# Drop nan values if any\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Remove duplicates\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "# Encoding categorical variables\n",
        "df = pd.get_dummies(df, columns=['sex', 'smoker', 'region'], drop_first=True)\n",
        "\n",
        "# Data spliting training 80% and test 20%\n",
        "X = df.drop('charges', axis=1)\n",
        "y = df['charges']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check sizes\n",
        "print(f'Training set: {X_train.shape}')\n",
        "print(f'Test set: {X_test.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYu_kCmvCyyS",
        "outputId": "0064e764-02c7-4be7-f897-861748ed0887"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set: (1069, 8)\n",
            "Test set: (268, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "load dataset from google drive, check for null values, drop if any, drop duplicates if any. Encode categorical data to binaries then split data to features and target variable, split train 80% and test 20%."
      ],
      "metadata": {
        "id": "Maq47ztIEs28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fitting the Models"
      ],
      "metadata": {
        "id": "5l45rR2ZFQ8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Dataset on LinearRegression\n",
        "model_lr = LinearRegression()\n",
        "model_lr.fit(X_train, y_train)\n",
        "\n",
        "y_pred_lr = model_lr.predict(X_test)\n",
        "\n",
        "# Run Dataset on Lasso\n",
        "model_lasso = Lasso(alpha=2.0)\n",
        "model_lasso.fit(X_train, y_train)\n",
        "\n",
        "y_pred_lasso = model_lasso.predict(X_test)\n",
        "\n",
        "# Run Dataset on MLPRegressor\n",
        "model_mlp = MLPRegressor(hidden_layer_sizes=(10, 5), max_iter=5000)\n",
        "model_mlp.fit(X_train, y_train)\n",
        "\n",
        "y_pred_mlp = model_mlp.predict(X_test)\n",
        "\n",
        "# Run Dataset on Gradient Boosting Regression\n",
        "model_gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "model_gbr.fit(X_train, y_train)\n",
        "\n",
        "y_pred_gbr = model_gbr.predict(X_test)\n",
        "\n",
        "# Run Dataset on SVR\n",
        "model_svr = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
        "model_svr.fit(X_train, y_train)\n",
        "\n",
        "y_pred_svr = model_svr.predict(X_test)"
      ],
      "metadata": {
        "id": "ROWCscS0FTjL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training on different algorithms\n",
        "- LinearRegression straight-forward has no hyperparamaters\n",
        "- Lasso takes a penatly of 2.0, the higher the penalty the better feature selection(reducing the weight to zero) in done on the dataset.\n",
        "- MLPRegressor takes two hidden_layers, 10 neurons and 5 neurons on each layer, trains on a max iteration of 5000 till model converges\n",
        "- GradientBoostingRegressor, takes 100 trees(n_estimators), very low learning_rate so the model has stable convergence, max_depth 3 for how complex level of the trees, random_state for reproducibility.\n",
        "- SVR takes the RBF so it suits non-linear relationships, C for penalty, chose 1 to avoid overfitting, epsilon the margin for error tolerance"
      ],
      "metadata": {
        "id": "eTXGkgM4Iqnk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Metrics"
      ],
      "metadata": {
        "id": "Gv-IdcP0LQZ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics on Test Data"
      ],
      "metadata": {
        "id": "uhF-1xl4LV-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the MSE for model predictions\n",
        "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
        "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
        "mse_neural = mean_squared_error(y_test, y_pred_mlp)\n",
        "mse_gradient = mean_squared_error(y_test, y_pred_gbr)\n",
        "mse_svr = mean_squared_error(y_test, y_pred_svr)\n",
        "\n",
        "print(f'Linear Regression - MSE: {mse_lr:.2f}')\n",
        "print(f'Lasso Regression - MSE: {mse_lasso:.2f}')\n",
        "print(f'Neural Network - MSE: {mse_neural:.2f}')\n",
        "print(f'Gradient Boosting - MSE: {mse_gradient:.2f}')\n",
        "print(f'SVR - MSE: {mse_svr:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2_RahcfLY2V",
        "outputId": "1c1fc31f-e58b-4c6a-dc7e-52344f6a5a07"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression - MSE: 35478020.68\n",
            "Lasso Regression - MSE: 35491022.54\n",
            "Neural Network - MSE: 36263054.07\n",
            "Gradient Boosting - MSE: 18218239.92\n",
            "SVR - MSE: 208462453.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean Squared Error for each Model, measures the average squared difference between actual and predicted values. Lower MSE means better model performance."
      ],
      "metadata": {
        "id": "PYJA2MwbMcAE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparion Analysis"
      ],
      "metadata": {
        "id": "X3qoB2u5MirV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **Mean Squared Error (MSE)** shows how far predictions are from actual values—the lower, the better. **Gradient Boosting performed best** with the lowest MSE (18M), meaning it made the most accurate predictions. **Linear Regression and Lasso Regression** had similar MSE (35M), showing that regularization (Lasso) didn’t help much. **Neural Network (MLP)** did slightly worse (~36M), possibly due to poor training or unsuitable data. **SVR performed the worst** (208M), likely due to bad hyperparameters or struggling with non-linear data. This suggests **Gradient Boosting is the best choice**, as it captures complex patterns better than the other models."
      ],
      "metadata": {
        "id": "X2MEikl_M7Js"
      }
    }
  ]
}